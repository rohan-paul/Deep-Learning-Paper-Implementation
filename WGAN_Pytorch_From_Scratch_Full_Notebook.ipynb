{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Is a Wasserstein GAN?\n",
        "\n",
        "It is an extension of the GAN that seeks an alternate way of training the generator model to better approximate the distribution of data observed in a given training dataset.\n",
        "\n",
        "### Wasserstein GAN, or WGAN, is a type of generative adversarial network that minimizes an approximation of the Earth-Mover's distance (EM) rather than the Jensen-Shannon divergence as in the original GAN formulation.\n",
        "\n",
        "#### Here in WGAN, the discriminator does not actually classify instances. Rather here for each instance the Discriminator outputs a number. This number does not have to be less than one or greater than 0, so we can't use 0.5 as a threshold to decide whether an instance is real or fake. Discriminator training just tries to make the output bigger for real instances than for fake instances.\n",
        "\n",
        "Instead of using a discriminator to classify or predict the probability of generated images as being real or fake, the WGAN changes or replaces the discriminator model with a critic that scores the realness or fakeness of a given image.\n",
        "\n",
        "This change is motivated by a mathematical argument that training the generator should seek a minimization of the distance between the distribution of the data observed in the training dataset and the distribution observed in generated examples. The argument contrasts different distribution distance measures, such as Kullback-Leibler (KL) divergence, Jensen-Shannon (JS) divergence, and the Earth-Mover (EM) distance, referred to as Wasserstein distance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The idea for the working of WGANs is to utilize two probability distributions. One is the probability distribution of the generator (Pg), which refers to the distribution from the output of the generator model.\n",
        "\n",
        "The other is the probability distribution from the real images (Pr).\n",
        "\n",
        "And the objective of WGAN is to ensure that both these probability distributions are close to each other so that the output generated is highly realistic and high-quality.\n",
        "\n",
        "For calculating the distance of these probability distributions, mathematical statistics in machine learning proposes three primary methods, namely\n",
        "\n",
        "- Kullback–Leibler divergence,\n",
        "- Jensen–Shannon divergence, and\n",
        "- Wasserstein distance.\n",
        "\n",
        "The Jensen–Shannon divergence (also a typical GAN loss) is the more utilized mechanism in simple GAN networks.\n",
        "\n",
        "#### But in WGAN, we use the Wasserstein distance (a.k.a Earth Mover’s Distance) instead of Jensen-Shannon Divergence to compare probability distributions.\n",
        "\n",
        "**The benefit of the WGAN is that the training process is more stable and less sensitive to model architecture and choice of hyperparameter configurations.**\n",
        "\n",
        "---\n",
        "\n",
        "## Compared to the original GAN algorithm, the WGAN undertakes the following changes:\n",
        "\n",
        "* After every gradient update on the critic function, clamp the weights to a small fixed range, [-c, c].\n",
        "\n",
        "* Use a new loss function derived from the Wasserstein distance, no logarithm anymore. The “discriminator” model does not play as a direct critic but a helper for estimating the Wasserstein metric between real and generated data distribution.\n",
        "\n",
        "* Empirically the authors recommended RMSProp optimizer on the critic, rather than a momentum based optimizer such as Adam which could cause instability in the model training.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Key Points in WGAN\n",
        "\n",
        "![Imgur](https://imgur.com/cWROjs7.png)\n",
        "\n",
        "### 1. Critic Weight Clipping\n",
        "\n",
        "The critic F has to be a 1-Lipschitz function. To enforce the constraint, WGAN applies a very simple clipping to restrict the maximum weight value in F,\n",
        "\n",
        "i.e. the weights of the discriminator must be within a certain range controlled by the hyperparameters c\n",
        "\n",
        "### 2. Update Critic More Than Generator\n",
        "\n",
        "In the DCGAN, the generator and the discriminator model must be updated in equal amounts.\n",
        "\n",
        "Specifically, the discriminator is updated with a half batch of real and a half batch of fake samples each iteration, whereas the generator is updated with a single batch of generated samples.\n",
        "\n",
        "In the WGAN model, the critic model must be updated more than the generator model.\n",
        "\n",
        "Specifically, a new hyperparameter is defined to control the number of times that the critic is updated for each update to the generator model, called n_critic.\n",
        "\n",
        "### 3. Use RMSProp Stochastic Gradient Descent\n",
        "\n",
        "The DCGAN uses the Adam version of stochastic gradient descent with a small learning rate and modest momentum.\n",
        "\n",
        "The WGAN recommends the use of Root Mean Square Propagation or RMSProp instead (which is one of the Adaptive Learning Rate Gradient Descent), with a small learning rate of 0.00005.\n",
        "\n",
        "---\n",
        "\n",
        "## The loss function for WGAN\n",
        "\n",
        "#### First, for a Normal GAN (e.g. DCGAN) the Loss definition is ;\n",
        "\n",
        "Critic Loss: D(x) - D(G(z))\n",
        "\n",
        "Where,\n",
        "- D(x) is the discriminator's estimate of the probability that real data instance x is real.\n",
        "- G(z) is the generator's output when given noise z.\n",
        "- D(G(z)) is the discriminator's estimate of the probability that a fake instance is real.\n",
        "\n",
        "\n",
        "#### Now for WGAN the Loss is defined as:\n",
        "\n",
        "#### Critic Loss = [average critic score on real images] – [average critic score on fake images]\n",
        "\n",
        "Critic Loss: D(x) - D(G(z))\n",
        "\n",
        "In WGAN, the Discriminator, does not produce a Probability, rather it produces a pure score.\n",
        "\n",
        "Where,\n",
        "    - D(x) is the critic's output for a real instance.\n",
        "    - G(z) is the generator's output when given noise z.\n",
        "    - D(G(z)) is the critic's output for a fake instance.\n",
        "\n",
        "The output of critic D does not have to be between 1 and 0.\n",
        "\n",
        "- The discriminator tries to maximize this function. In other words, it tries to maximize the difference between its output on real instances and its output on fake instances.\n",
        "\n",
        "- So, when compared to the Normal GAN's Discriminator, the Discriminator in WGAN, we do NOT classify or predict the probability of generated images as being real or fake. Instead, the WGAN replaces the discriminator model with a critic that scores the realness or fakeness of a given image.\n",
        "\n",
        "- It does this by removing the last Sigmoid() layer and have a linear layer at the end of the discriminator’s neural network.\n",
        "\n",
        "#### Generator Loss = -[average critic score on fake images]\n",
        "\n",
        "Generator Loss: D(G(z))\n",
        "\n",
        "The generator tries to maximize this function. In other words, It tries to maximize the discriminator's output for its fake instances. In these functions:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Implementing Wasserstein Loss\n",
        "\n",
        "1.  Use a linear activation function in the output layer of the critic model (instead of sigmoid).\n",
        "\n",
        "2. Use Wasserstein loss to train the critic and generator models that promote larger difference between scores for real and generated images.\n",
        "\n",
        "3. Constrain critic model weights to a limited range after each mini batch update (e.g. [-0.01,0.01]).\n",
        "\n",
        "4. In order to have parameters w lie in a compact space, something simple we can do is clamp the weights to a fixed box (say W = [−0.01, 0.01]l ) after each gradient update.\n",
        "\n",
        "\n",
        "We can summarize the function as it is described in the paper as follows:\n",
        "\n",
        "\n",
        "#### Critic Loss = [average critic score on real images] – [average critic score on fake images]\n",
        "\n",
        "#### Generator Loss = -[average critic score on fake images]\n",
        "\n",
        "Where the average scores are calculated across a mini-batch of samples.\n",
        "\n",
        "The calculations are straightforward to interpret once we recall that stochastic gradient descent seeks to minimize loss.\n",
        "\n",
        "#### In the case of the generator, a larger score from the critic will result in a smaller loss for the generator, encouraging the critic to output larger scores for fake images. For example, an average score of 10 becomes -10, an average score of 50 becomes -50, which is smaller, and so on.\n",
        "\n",
        "#### In the case of the critic, a larger score for real images results in a larger resulting loss for the critic, penalizing the model. This encourages the critic to output smaller scores for real images. For example, an average score of 20 for real images and 50 for fake images results in a loss of -30; an average score of 10 for real images and 50 for fake images results in a loss of -40, which is better, and so on.\n",
        "\n",
        "#### The sign of the loss does not matter in this case, as long as loss for real images is a small number and the loss for fake images is a large number. The Wasserstein loss encourages the critic to separate these numbers.\n",
        "\n",
        "#### We can also reverse the situation and encourage the critic to output a large score for real images and a small score for fake images and achieve the same result.\n",
        "\n",
        "---\n",
        "\n",
        "### Main Equation\n",
        "\n",
        "The network uses Earth Mover’s Distance instead of Jensen-Shannon Divergence to compare probability distributions.\n",
        "\n",
        "![Imgur](https://imgur.com/EJg4nHM.png)\n",
        "\n",
        "In the above equation, the max value represents the constraint on the discriminator. In the WGAN architecture, the discriminator is referred to as the critic. One of the reasons for this convention is that there is no sigmoid activation function to limit the values to 0 or 1, which means real or fake. So the discriminator in WGAN, outputs a scalar score rather than a probability.\n",
        "\n",
        "The first part of the equation represents the real data, while the second half represents the generator data. The discriminator (or the critic) in the above equation aims to maximize the distance between the real data and the generated data, because it wants to be able to successfully distinguish the data accordingly.\n",
        "\n",
        "The generator network aims to minimize the distance between the real data and generated data because it wants the generated data to be as real as possible.\n",
        "\n",
        "---\n",
        "\n",
        "## Jensen Shannon Divergence (JSD)\n",
        "\n",
        "\n",
        "The objective function of our original GAN is essentially the minimization of something called the Jensen Shannon Divergence (JSD). Specifically it is:\n",
        "\n",
        "![Imgur](https://imgur.com/kYc2Cfv.png)\n",
        "\n",
        "---\n",
        "\n",
        "Sadly, Wasserstein GAN is not perfect. Even the authors of the original WGAN paper mentioned that “Weight clipping is a clearly terrible way to enforce a Lipschitz constraint” (Oops!). WGAN still suffers from unstable training, slow convergence after weight clipping (when clipping window is too large), and vanishing gradients (when clipping window is too small).\n",
        "\n",
        "Some improvement, precisely replacing weight clipping with gradient penalty is one of the most prominent solution that has been proposed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A1WwPqxmfrE"
      },
      "source": [
        "## Implementation from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzolhsHfVzWO",
        "outputId": "0d3b2a3f-d1d4-45bc-c091-808931832607",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7Aa7VNjLmaw9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from tqdm import tqdm \n",
        "\n",
        "plt.ion()\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ9BWJV3WYUt",
        "outputId": "6018a674-99da-4eea-f989-9d4e5a753762",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Mar 22 20:37:53 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Hyperparameters(object):\n",
        "      def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "hp = Hyperparameters(n_epochs=200,\n",
        "                     batch_size=64,\n",
        "                     lr=0.00005,                     \n",
        "                     n_cpu=8,\n",
        "                     latent_dim=100,\n",
        "                     img_size=32,\n",
        "                     channels=1,\n",
        "                     n_critic=25,\n",
        "                     clip_value=.005,\n",
        "                     sample_interval=400)\n",
        "\n",
        "print(hp.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wQcu6fi-WPCF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "root_path = '/content/drive/MyDrive/All_Datasets/Fashion_MNIST'\n",
        "''' The Fashion-MNIST dataset contains 60,000 training images (and 10,000 test images) of fashion and clothing items, taken from 10 classes. Each image is a standardized 28×28 size in grayscale (784 total pixels). '''\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "  datasets.FashionMNIST(\n",
        "    root_path,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.Resize(hp.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "    ),\n",
        "  ),\n",
        "  batch_size=hp.batch_size,\n",
        "  shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ954M71otK9"
      },
      "source": [
        "SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dbDFpxOdonQI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# os.makedirs(\"images\", exist_ok=True)\n",
        "img_shape = (hp.channels, hp.img_size, hp.img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "def weights_init_normal(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find(\"Conv\") != -1:\n",
        "    torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "  elif classname.find(\"BatchNorm2d\") != -1:\n",
        "    torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "def to_img(x):\n",
        "  x = x.clamp(0, 1)\n",
        "  return x\n",
        "\n",
        "def visualise_output(images, x, y):\n",
        "  with torch.no_grad():  \n",
        "    images = images.cpu()\n",
        "    images = to_img(images)\n",
        "    np_imagegrid = make_grid(images, x, y).numpy()\n",
        "    figure(figsize=(20,20))\n",
        "    plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIXAbyxIo9r9"
      },
      "source": [
        "## GENERATOR\n",
        "\n",
        "The generator model takes as input a point in the latent space and outputs a single 28×28 grayscale image.\n",
        "\n",
        "This is achieved by using a fully connected layer to interpret the point in the latent space. \n",
        "\n",
        "This is then upsampled couple of more times, doubling the size.\n",
        "\n",
        "### np.prod()\n",
        "\n",
        "Return the product of array elements over a given axis.\n",
        "\n",
        "If the input array is blank, then this method returns the neutral element: 1\n",
        "\n",
        "By default, the axis is set to None, thereby calculating the product of all the elements in the given array. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bTfDyWs-o_cG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_shape, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_features, out_features, normalize=True):\n",
        "            layers = [nn.Linear(in_features, out_features)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_features, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(\n",
        "                in_features=latent_dim, out_features=128, normalize=False\n",
        "            ),  # Batch_size, 784 -> Batch_size, 128\n",
        "            *block(\n",
        "                in_features=128, out_features=256\n",
        "            ),  # Batch_size, 128 -> Batch_size, 256\n",
        "            *block(\n",
        "                in_features=256, out_features=512\n",
        "            ),  # Batch_size, 256 -> Batch_size, 512\n",
        "            *block(\n",
        "                in_features=512, out_features=1024\n",
        "            ),  # Batch_size, 512 -> Batch_size, 1024\n",
        "            nn.Linear(\n",
        "                in_features=1024, out_features=int(np.prod(img_shape))\n",
        "            ),  # Batch_size, 1024 -> Batch_size, np.prod(img_shape)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, img_shape, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.shape[0], *img_shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P01xoE6FpL9-"
      },
      "source": [
        "## DISCRIMINATOR\n",
        "\n",
        "One of the reasons for this convention is that there is no sigmoid activation function to limit the values to 0 or 1, which means real or fake. So the discriminator in WGAN, outputs a scalar score rather than a probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jYlPm9G8pN6u",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, img_shape):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=int(np.prod(img_shape)), out_features=512\n",
        "            ),  # Batch_size, np.prod(img_shape) -> Batch_size, 512\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(\n",
        "                in_features=512, out_features=256\n",
        "            ),  # Batch_size, 512 -> Batch_size, 256\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(\n",
        "                in_features=256, out_features=1\n",
        "            ),  # Batch_size, 256 -> Batch_size, 1\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.shape[0], -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y-0F1URpfQd"
      },
      "source": [
        "LOSS and MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UPqaoCSphQ1",
        "outputId": "dc9deab8-1c97-49c7-c4bb-d9a05ac7b52a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Critic(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator = Generator(img_shape, hp.latent_dim)\n",
        "critic = Critic(img_shape)\n",
        "\n",
        "if cuda:\n",
        "  generator.cuda()\n",
        "  critic.cuda()  \n",
        "\n",
        "# Initialize weights\n",
        "generator.apply(weights_init_normal)\n",
        "critic.apply(weights_init_normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0X-PJWYp6W9"
      },
      "source": [
        "OPTIMIZERS and TENSOR SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ymTI34yKqA2u",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=hp.lr)\n",
        "optimizer_D = torch.optim.RMSprop(critic.parameters(), lr=hp.lr)\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf3lQ1-0qehW"
      },
      "source": [
        "## TRAINING STEPS\n",
        "\n",
        "1. The critic network is first trained on a real batch of data, then trained on a batch of data generated from a noise-prior via the generator. \n",
        "\n",
        "2. The critic's loss function is arranged such that it estimates the Wasserstein Distance (maximizes the distance between the two distributions) then clips its own weights to ensure it is 1-Lipschitz-Continuous. \n",
        "\n",
        "3. Then, the generator generates a new batch of images from a noise prior, passes these through to the critic who then \"informs\" the generator of the Wasserstein-1 distance between the true distribution and the distribution of the images the Generator just created. \n",
        "\n",
        "4. It does this via the loss function of the critic. The critic's weights are frozen and the error propagates all the way back through to the generator who then updates its parameters to minimize the Wasserstein distance. \n",
        "\n",
        "5. This repeats until the loss (hopefully) converges to near zero and the distributions are approximately equal.\n",
        "\n",
        "6. The discriminator loss is (an approximation of) the negative Wasserstein distance between the generator distribution and the data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ipLMbwcxqgPm",
        "outputId": "25fcb2db-224b-4c8f-d39c-b523685f2593",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "for epoch in range(hp.n_epochs):\n",
        "  for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "      # Adversarial ground truths\n",
        "      valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "      # Configure input\n",
        "      real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "      # -----------------\n",
        "      #  Train Critic\n",
        "      # -----------------\n",
        "\n",
        "      optimizer_G.zero_grad()\n",
        "\n",
        "      # Sample noise as generator input\n",
        "      # Draw random samples from a normal (Gaussian) distribution.\n",
        "      # np.random.normal(mean, sd, Output shape)\n",
        "      z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], hp.latent_dim))))\n",
        "\n",
        "      # Generate a batch of images\n",
        "      fake_imgs = generator(z).detach()\n",
        "\n",
        "      ''' The math for the loss functions for the critic and generator is:\n",
        "        Critic Loss: D(x) - D(G(z))\n",
        "        Generator Loss: D(G(z))\n",
        "        Now for the Critic Loss, as per the Paper, we have to maximize the expression.\n",
        "        So, arithmetically, maximizing an expression, means minimizing the -ve of that expression\n",
        "        i.e. -(D(x) - D(G(z))) which is -D(x) + D(G(z)) i.e. -D(real_imgs) + D(G(real_imgs))\n",
        "     '''\n",
        "      d_loss = -torch.mean(critic(real_imgs)) + torch.mean(critic(fake_imgs)) \n",
        "\n",
        "      d_loss.backward()\n",
        "      optimizer_D.step()\n",
        "\n",
        "      \n",
        "      ''' Clip weights of critic to avoid vanishing/exploding gradients in the \n",
        "      critic/critic. \n",
        "      In order to have parameters w lie in a compact space, something simple we can do is clamp the weights to a fixed box (say W = [-0.005, 0.005]l ) after each gradient update.\n",
        "      \n",
        "      torch.clamp() is used to clamp all the elements in an input into the range [min, max]. It takes three parameters: the input tensor, min, and max values. The values less than the min are replaced by the min and the values greater than the max are replaced by the max. If min is not given, then there is no lower bound. If max is not given, then there is no upper bound.  '''\n",
        "      for p in critic.parameters():\n",
        "        p.data.clamp_(-hp.clip_value, hp.clip_value)\n",
        "\n",
        "\n",
        "      ''' Train the generator every n_critic iterations \n",
        "      we need to increase training iterations of the critic so that it works to \n",
        "      approximate the real distribution sooner.\n",
        "      '''\n",
        "      if i % hp.n_critic == 0:\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_images_from_generator = generator(z)\n",
        "        # Adversarial loss\n",
        "        g_loss = -torch.mean(critic(fake_images_from_generator))\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()    \n",
        "\n",
        "      batches_done = epoch * len(dataloader) + i\n",
        "      if batches_done % hp.sample_interval == 0:\n",
        "        clear_output()\n",
        "        print(f\"Epoch:{epoch}:It{i}:DLoss{d_loss.item()}:GLoss{g_loss.item()}\")          \n",
        "        visualise_output(fake_images_from_generator.data[:50],10, 10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GEN_4_WGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
